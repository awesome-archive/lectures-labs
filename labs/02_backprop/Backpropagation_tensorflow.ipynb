{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multilayer Neural Networks in TensorFlow\n",
    "\n",
    "### Goals: \n",
    "- Auto-differentiation: the basics of `TensorFlow`\n",
    "\n",
    "### Dataset:\n",
    "- Similar as first Lab - Digits: 10 class handwritten digits\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# display figures in the notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# print(scaler.mean_)\n",
    "# print(scaler.scale_)\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Implementation\n",
    "\n",
    "TensorFlow is a dynamic graph computation engine, that allows automatic differentiation of each node. Tensorflow is the default computational backend of the Keras library. I can also be used directly from Python to build deep learning models.\n",
    "\n",
    "- https://www.tensorflow.org \n",
    "- https://www.tensorflow.org/tutorials/mnist/tf/\n",
    "\n",
    "TensorFlow builds where nodes may be:\n",
    "- **constant:** constants tensors, such as a learning rate\n",
    "- **Variables:** any tensor, such as parameters of the models\n",
    "\n",
    "**Note** that we are using for this course the new version Tensorflow 2.0. This version cleaned the old cluttered api and ditch the static graph to a dynamic graph that is more easy to use. Previously you defined the graph once, and then needed to evaluate it. Now it acts as Python code, where you `print` and use `pdb.set_trace()` to inspect intermediary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(3)\n",
    "b = tf.constant(2)\n",
    "c = tf.Variable(0)\n",
    "c = a + b\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tf.Tensor can be converted to numpy the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explicitely place tensors on a device, use context managers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"CPU:0\"):\n",
    "    x_cpu = tf.constant(3)\n",
    "    \n",
    "# with tf.device(\"GPU:0\"):\n",
    "#     x_gpu = tf.constant(3)\n",
    "x_cpu.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: batches in inputs**\n",
    "- the first dimension of the input is usually kept for the batch dimension. A typical way to define an input placeholder with a 1D tensor of 128 dimensions, is:\n",
    "```\n",
    "X = tf.placeholder(\"float32\", shape=[None, 128])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Build a model using TensorFlow\n",
    "\n",
    "- Using TensorFlow, build a similar model (one hidden layer) as you previously did\n",
    "- the input will be a batch coming from X_train, and the output will be a batch of ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random.normal(shape, stddev=0.01))\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y):\n",
    "    return np.mean(np.argmax(y_pred, axis=1) == y)\n",
    "\n",
    "\n",
    "def gen_dataset(x, y, batch_size=128):\n",
    "    return tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size=batch_size)\n",
    "\n",
    "def test_model(model, x, y):\n",
    "    dataset = gen_dataset(x, y)\n",
    "    preds, targets = [], []\n",
    "    \n",
    "    for batch_x, batch_y in dataset:\n",
    "        preds.append(model(batch_x).numpy())\n",
    "        targets.append(batch_y.numpy())\n",
    "     \n",
    "    preds, targets = np.concatenate(preds), np.concatenate(targets)\n",
    "    return accuracy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = gen_dataset(X_train, y_train)\n",
    "print(dataset)\n",
    "\n",
    "batch_x, batch_y = next(iter(dataset))\n",
    "batch_x.shape, batch_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your model there, and then execute the following cell to train your model.\n",
    "Don't hesitate to tweak the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "batch_size = 32\n",
    "hid_size = 15\n",
    "learning_rate = 0.5\n",
    "num_epochs = 10\n",
    "input_size = X_train.shape[1]\n",
    "output_size = 10\n",
    "\n",
    "# build the model and weights\n",
    "class MyModel:\n",
    "    def __init__(self, input_size, hid_size, output_size):\n",
    "        self.W_h = None # TODO\n",
    "        self.b_h = None # TODO\n",
    "        self.W_o = None # TODO\n",
    "        self.b_o = None # TODO\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        # TODO\n",
    "        return None\n",
    "    \n",
    "model = MyModel(input_size, hid_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/tf_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for e in range(num_epochs):\n",
    "    train_dataset = gen_dataset(X_train, y_train, batch_size=batch_size)\n",
    "    \n",
    "    for batch_x, batch_y in train_dataset:\n",
    "        # tf.GradientTape records the activation to compute the gradients:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(batch_x)\n",
    "            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(batch_y, logits))\n",
    "            losses.append(loss.numpy())\n",
    "            \n",
    "        # Here we ask for the gradients of dL/dW_h, etc.\n",
    "        dW_h, db_h, dW_o, db_o = tape.gradient(\n",
    "            loss, [model.W_h, model.b_h, model.W_o, model.b_o])\n",
    "        \n",
    "        # Update the weights as a Stochastic Gradient Descent would do:\n",
    "        model.W_h.assign_sub(learning_rate * dW_h)\n",
    "        model.b_h.assign_sub(learning_rate * db_h)\n",
    "        model.W_o.assign_sub(learning_rate * dW_o)\n",
    "        model.b_o.assign_sub(learning_rate * db_o)\n",
    "        \n",
    "    train_acc = test_model(model, X_train, y_train)\n",
    "    test_acc = test_model(model, X_test, y_test)\n",
    "    print(\"Epoch {}, train_acc = {}, test_acc = {}\".format(e, round(train_acc, 4), round(test_acc, 4)))\n",
    "    \n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Exercises\n",
    "\n",
    "### Bonus:\n",
    "- add L2 regularization with $\\lambda = 10^{-4}$\n",
    "- train with arbitrary number of layers by only defining layer sizes\n",
    "- try implementing momentum\n",
    "- you may use tensorboard (https://www.tensorflow.org/how_tos/summaries_and_tensorboard/) to monitor loss and display graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
